# -*- coding: utf-8 -*-
"""NER JSON

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T5W_TLwgkY6_OJcHSYoH6SheFrC8HVLe

## Step 0: Load Libraries
"""

!pip install openai datasets transformers

# !pip install openai
!pip install datasets

import openai
import os

import json

from transformers import BertModel, BertConfig, BertTokenizer

from datasets import load_dataset

"""## Step 1: Setup APIs and Dataset

"""

# Commented out IPython magic to ensure Python compatibility.
# %env OPENAI_API_KEY = {}#include key here
openai.api_key = os.environ['OPENAI_API_KEY']

conll = load_dataset("conll2003")
wnut = load_dataset("wnut_17")

conll_idx_ner_tags = {0:'O', 1:'B-PER', 2:'I-PER', 3:'B-ORG', 4:'I-ORG', 5:'B-LOC', 6:'I-LOC', 7:'B-MISC', 8:'I-MISC'}
wnut_idx_ner_tags = {0: 'O', 1: 'B-corporation', 2: 'I-corporation', 3: 'B-creative-work', 4: 'I-creative-work', 5: 'B-group', 6: 'I-group', 7: 'B-location', 8: 'I-location', 9: 'B-person', 10: 'I-person', 11: 'B-product', 12: 'I-product'}

def get_input_output(data, idx_ner_tags):
  tokens = data["tokens"]
  ner_labels = data["ner_tags"]

  mod_tokens = [idx_ner_tags[ner_labels[i]] for i in range(len(tokens))]

  input = " ".join(tokens)
  output = " ".join(mod_tokens)

  return input, output

def generate_json(data, filename, tag_roots, idx_ner_tags):
  named_entities = {}
  named_entities["tags"] = tag_roots
  named_entities["sentences"] = []

  #tag_roots = ["PER", "ORG", "LOC", "MISC"]

  for s in range(len(data)):
    sentence = data[s]
    N = len(sentence["tokens"])

    named_entity = {}

    #Generate Sentences + Their Labels
    tokens = sentence["tokens"]
    ner_labels = sentence["ner_tags"]

    ner_tokens = [idx_ner_tags[ner_labels[i]] for i in range(len(tokens))]

    #named_entity["tokens"] = tokens
    #named_entity["ner_tokens"] = ner_tokens

    named_entity["id"] = s

    named_entity["sentence"] = " ".join(tokens)
    named_entity["ner_sentence"] = " ".join(ner_tokens)

    #Generate Entities
    named_entity["entities"] = {}

    for root in tag_roots:
      named_entity['entities'][root] = []

    for i in range(N):
      for root in tag_roots:
        btag = "B-" + root

        if idx_ner_tags[sentence["ner_tags"][i]] == btag:
          j = i + 1

          itag = "I-" + root

          while j < N and idx_ner_tags[sentence["ner_tags"][j]] == itag:
            j += 1

          named_entity['entities'][root].append(" ".join(sentence["tokens"][i:j]))

    named_entities["sentences"].append(named_entity)

  with open(filename, "w") as outfile:
      json.dump(named_entities, outfile)

conll_tag_roots = ["PER", "ORG", "LOC", "MISC"]

generate_json(conll["train"], "conll_train.json", conll_tag_roots, conll_idx_ner_tags)
generate_json(conll["test"], "conll_test.json", conll_tag_roots, conll_idx_ner_tags)
generate_json(conll["validation"], "conll_val.json", conll_tag_roots, conll_idx_ner_tags)

wnut_tag_roots = ["corporation", "creative-work", "group", "location", "person", "product"]

generate_json(wnut["train"], "wnut_train.json", wnut_tag_roots, wnut_idx_ner_tags)
generate_json(wnut["test"], "wnut_test.json", wnut_tag_roots, wnut_idx_ner_tags)
generate_json(wnut["validation"], "wnut_val.json", wnut_tag_roots, wnut_idx_ner_tags)

"""## Step 2: Test Prompting"""



"""## Step 3: Prompt Tuning/Few Shot Learning w Prompts"""



"""## Step 4: Maybe -- Finetuning Network"""



"""## Step 5: Collect Results"""



