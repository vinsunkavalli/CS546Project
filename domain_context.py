# -*- coding: utf-8 -*-
"""CS546_task_modification_with_and_without_domain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17bSFvx4PJjuVBXHDX4XhSE3cc-CzwPzb
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir("/content/drive/MyDrive/Programming_Assistent/CS546 Final Project")

!pip install openai

!pip install openai datasets transformers

import openai
import os
from datasets import load_dataset #CoNLL, Ontonotes, and Wnut17 are all potential NER datasets to use

conll = load_dataset("conll2003")
wnut = load_dataset("wnut_17")

conll_idx_ner_tags = {0:'O', 1:'B-PER', 2:'I-PER', 3:'B-ORG', 4:'I-ORG', 5:'B-LOC', 6:'I-LOC', 7:'B-MISC', 8:'I-MISC'}
wnut_idx_ner_tags = {0: 'O', 1: 'B-corporation', 2: 'I-corporation', 3: 'B-creative-work', 4: 'I-creative-work', 5: 'B-group', 6: 'I-group', 7: 'B-location', 8: 'I-location', 9: 'B-person', 10: 'I-person', 11: 'B-product', 12: 'I-product'}

from openai import OpenAI
client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="",
)

def generate_prompt(prompt_data):
  prompt = "Please generate Named Entity Recognition (NER) labels based on entity type for the last sentence, similar to prior examples:\n\n"
  prompt += "Examples:\n"
  answer = None

  for data in prompt_data:
    input, output = data
    prompt += f"Input: {input}\n"
    prompt += f"Output: {output}\n\n"

  prompt += "Generate Named Entity Recognition (NER) labels for the following text:\n"
  return prompt

train_sentences = [(entry['sentence'],entry['ner_sentence']) for entry in train_data["sentences"]]
train_sentences=train_sentences[5:10]
prompt=generate_prompt(train_sentences)
print(prompt)

import json
with open("conll_train.json", 'r') as file:
    train_data = json.load(file)

import json
with open("conll_test.json", 'r') as file:
    test_data = json.load(file)

import openai

# Example data for domain classification
def get_domain(data):
  gpt_prompt = "what is the domain of following text in maximum 10 words:\n" + data
  response = client.chat.completions.create(
      model="gpt-3.5-turbo",
      temperature=0.5,
      messages=[
          {"role": "system", "content": "You are a helpful assistant."},
          {"role": "user", "content": gpt_prompt},
      ]
  )
  generated_domain = vars(vars(vars(response)['choices'][0])['message'])['content'].strip()

  return generated_domain[:-1]

"""##Without Domain"""

SYSTEM_PROMPT = "You are a smart and intelligent Named Entity Recognition (NER) system. I will provide you the definition of the entities \\
you need to extract and the sentence from which you need to extract the entities and the output in given format with examples."

USER_PROMPT_1 = "Are you clear about your role?"

ASSISTANT_PROMPT_1 = "Sure, I'm ready to help you with your NER task. Please provide me with the necessary information to get started."

PROMPT = (
    "Please generate Named Entity Recognition (NER) labels based on entity type for the last sentence, given the tags and examples below:\n"
    "Tags:\n"
    "{}\n\n"
    "Entity Definition:\n"
    "1. PER: Short name or full name of a person from any geographic regions.\n"
    "2. ORG: Name of the companies like Google, Samsung, Apple etc.\n"
    "3. LOC: Name of any geographic location, like cities, countries, continents, districts etc.\n"
    "4. MISC: Entities that don't fit into the above categories.\n"
    "\n"
    "Output Format:\n"
    "'PERSON': [list of entities present], 'ORG': [list of entities present], 'LOC': [list of entities present],'MISC': [list of entities present]\n"
    "\n"
    "Examples:\n"
    "{}\n"
    "Now, generate Named Entity Recognition (NER) labels for the following text:\n"
    "{}\n"
    "Output: "
)

"""corporation: Indicates named entities associated with corporations, companies or businesses.
"creative-work": Represents named entities related to creative works, such as books, movies, or songs.
"group": Indicates named entities associated with groups or organizations.
"location": Represents named entities related to locations or places.
"person": Denotes named entities referring to individuals or people.
"product": Denotes named entities associated with products or items.
"""

def generate_ner(prompt):
  response = client.chat.completions.create(
      model="gpt-3.5-turbo",
      temperature=0.5,
      messages=[
          {"role": "system", "content": SYSTEM_PROMPT},
          {"role": "user", "content": USER_PROMPT_1},
          {"role": "assistant", "content": ASSISTANT_PROMPT_1},
          {"role": "user", "content": prompt},
      ]
  )

  generated_summary = vars(vars(vars(response)['choices'][0])['message'])['content'].strip()

  return generated_summary

!pip install pinecone-client transformers openai datasets

from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch

tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER")
model = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER")

def get_embedding(model, tokenizer, sentence):
    #sentence_embeddings = []
    model.eval()
    with torch.no_grad():
      inputs = tokenizer(sentence, return_tensors="pt")
      outputs = model(**inputs, output_hidden_states=True)
      token_vecs = outputs.hidden_states[-2][0]
      sentence_embedding = torch.mean(token_vecs, dim=0).tolist()
      #sentence_embeddings.append(sentence_embedding.tolist())

    return sentence_embedding

import pinecone

pinecone.init(
    api_key="f8c61eed-169b-45a6-b16f-d9a3fb0daae8",
    environment="gcp-starter"
)

index = pinecone.Index('bert-ner-embed')

import random

"""
For our baseline, we plan to use randomly selected training sentences as examples
"""
def generate_examples_knn_task_mod(input_sentence, train_data, k=5, mode="label"):#, search_type="random"):
  embed = get_embedding(model, tokenizer, input_sentence)
  query = index.query(vector=embed, top_k=k)
  matches = query['matches']
  ids = [int(matches[i]['id'][6:]) for i in range(len(matches))]

  examples_str = ""

  for id in ids:
    examples_str += "Input: "
    examples_str += train_data[id]["sentence"]
    examples_str += "\n"
    examples_str += "Output: "
    examples_str += str(train_data[id]["entities"])
    examples_str += "\n\n"

  return examples_str
  #pass
#print(generate_examples_knn_task_mod(train_data["sentences"][0]["sentence"], train_data["sentences"], 5))

tags = test_data["tags"]
input_data = test_data["sentences"]
example_data = train_data["sentences"]
prompt = None
asnwer = None
task_modification = {"predictions":[], "labels":[]}
for i in range(len(test_data["sentences"])):
  print(i)
  input = input_data[i]["sentence"]
  output = input_data[i]["entities"]
  input = input_data[i]["sentence"]
  examples = generate_examples_knn_task_mod(input,example_data, k=5)
  prompt = PROMPT.format(tags, examples, input)
  answer = output
  generated_summary=generate_ner(prompt)
  print("answer:",answer)
  print("generated_summary",generated_summary)
  task_modification["predictions"].append(generated_summary)
  task_modification["labels"].append(answer)

import json
task_modification_result = json.dumps(task_modification, indent=2)
with open("task_modification_result.json", 'w') as json_file:
    json_file.write(task_modification_result)

"""## With Domain"""

import openai

# Example data for domain classification
def get_domain(data):
  gpt_prompt = "what is the domain of following text in maximum 10 words:\n" + data
  response = client.chat.completions.create(
      model="gpt-3.5-turbo",
      temperature=0.5,
      messages=[
          {"role": "system", "content": "You are a helpful assistant."},
          {"role": "user", "content": gpt_prompt},
      ]
  )
  generated_domain = vars(vars(vars(response)['choices'][0])['message'])['content'].strip()

  return generated_domain[:-1]

SYSTEM_PROMPT = "You are a smart and intelligent Named Entity Recognition (NER) system. I will provide you the definition of the entities you need to extract and the sentence from which you need to extract the entities and the output in given format with examples."

USER_PROMPT_1 = "Are you clear about your role?"

ASSISTANT_PROMPT_1 = "Sure, I'm ready to help you with your NER task. Please provide me with the necessary information to get started."
Prompt_domain = (
    "Please generate Named Entity Recognition (NER) labels based on entity type for the last sentence considering the domain of the text, given the tags and examples below:\n"
    "Tags:\n"
    "{}\n\n"
    "Entity Definition:\n"
    "1. PER: Short name or full name of a person from any geographic regions.\n"
    "2. ORG: Name of the companies like Google, Samsung, Apple etc.\n"
    "3. LOC: Name of any geographic location, like cities, countries, continents, districts etc.\n"
    "4. MISC: Entities that don't fit into the above categories.\n"
    "\n"
    "Output Format:\n"
    "'PERSON': [list of entities present], 'ORG': [list of entities present], 'LOC': [list of entities present],'MISC': [list of entities present]\n"
    "\n"
    "Examples:\n"
    "{}\n"
    "Now, generate Named Entity Recognition (NER) labels for the text related to {} domain the following text:\n"
    "{}\n"
    "Output: "
)

tags = test_data["tags"]
input_data = test_data["sentences"]
example_data = train_data["sentences"]
prompt = None
asnwer = None
with_domain_test = {"predictions":[], "labels":[]}
for i in range(100):
  print(i)
  input = input_data[i]["sentence"]
  output = input_data[i]["entities"]
  examples = generate_examples_knn_task_mod(input,example_data, k=5)
  domain=get_domain(input)
  prompt = Prompt_domain.format(tags, examples, domain, input)
  answer = output
  generated_summary=generate_ner(prompt)
  print("answer:",answer)
  print("generated_summary",generated_summary)
  with_domain_test["predictions"].append(generated_summary)
  with_domain_test["labels"].append(answer)

import json
with_domain_task_modification_result = json.dumps(with_domain_test, indent=2)
with open("with_domain_task_modification_result.json", 'w') as json_file:
    json_file.write(with_domain_task_modification_result)

"""## Without Domain wnut_17 dataset"""

import json
with open("wnut_train.json", 'r') as file:
    train_data = json.load(file)

import json
with open("wnut_test.json", 'r') as file:
    test_data = json.load(file)

SYSTEM_PROMPT = "You are a smart and intelligent Named Entity Recognition (NER) system. I will provide you the definition of the entities you need to extract and the sentence from which you need to extract the entities and the output in given format with examples."

USER_PROMPT_1 = "Are you clear about your role?"

ASSISTANT_PROMPT_1 = "Sure, I'm ready to help you with your NER task. Please provide me with the necessary information to get started."
PROMPT = (
    "Please generate Named Entity Recognition (NER) labels based on entity type for the last sentence, given the tags and examples below:\n"
    "Tags:\n"
    "{}\n\n"
    "Entity Definition:\n"
    "1. corporation: Indicates named entities associated with corporations, companies or businesses.\n"
    "2. creative-work: Represents named entities related to creative works, such as books, movies, or songs.\n"
    "3. group: Indicates named entities associated with groups or organizations.\n"
    "4. location: Name of any geographic location, like cities, countries, continents, districts etc.\n"
    "5. person: Short name or full name of a person from any geographic regions.\n"
    "6. product: Denotes named entities associated with products or items.\n"
    "\n"
    "Output Format:\n"
    "'corporation': [list of entities present], 'creative-work': [list of entities present], 'group': [list of entities present],'location': [list of entities present],'person': [list of entities present],'product': [list of entities present]\n"
    "\n"
    "Examples:\n"
    "{}\n"
    "Now, generate Named Entity Recognition (NER) labels for the following text:\n"
    "{}\n"
    "Output: "
)

import pinecone

pinecone.init(
	api_key='0a7ff5f2-254f-4aff-b399-0615282eaf8c',
	environment='us-central1-gcp'
)

index = pinecone.Index('wnut-ner-embed')

import random

"""
For our baseline, we plan to use randomly selected training sentences as examples
"""
def generate_examples_knn_task_mod(input_sentence, train_data, k=5, mode="label"):#, search_type="random"):
  embed = get_embedding(model, tokenizer, input_sentence)
  query = index.query(vector=embed, top_k=k)
  matches = query['matches']
  ids = [int(matches[i]['id'][5:]) for i in range(len(matches))]

  examples_str = ""

  for id in ids:
    examples_str += "Input: "
    examples_str += train_data[id]["sentence"]
    examples_str += "\n"
    examples_str += "Output: "
    examples_str += str(train_data[id]["entities"])
    examples_str += "\n\n"

  return examples_str

tags = test_data["tags"]
input_data = test_data["sentences"]
example_data = train_data["sentences"]
prompt = None
asnwer = None
task_modification = {"predictions":[], "labels":[]}
for i in range(len(test_data["sentences"])):
  print(i)
  input = input_data[i]["sentence"]
  output = input_data[i]["entities"]
  examples = generate_examples_knn_task_mod(input,example_data, k=5)
  prompt = PROMPT.format(tags, examples, input)
  answer = output
  generated_summary=generate_ner(prompt)
  print("answer:",answer)
  print("generated_summary",generated_summary)
  task_modification["predictions"].append(generated_summary)
  task_modification["labels"].append(answer)

task_modification_wnut_result = json.dumps(task_modification, indent=2)
import json
with open("task_modification_wnut_result.json", 'w') as json_file:
    json_file.write(task_modification_wnut_result)

"""##With Domain Wnut dataset"""

SYSTEM_PROMPT = "You are a smart and intelligent Named Entity Recognition (NER) system. I will provide you the definition of the entities you need to extract and the sentence from which you need to extract the entities and the output in given format with examples."

USER_PROMPT_1 = "Are you clear about your role?"

ASSISTANT_PROMPT_1 = "Sure, I'm ready to help you with your NER task. Please provide me with the necessary information to get started."
Prompt_domain = (
    "Please generate Named Entity Recognition (NER) labels based on entity type for the last sentence considering the domain of the text, given the tags and examples below:\n"
    "Tags:\n"
    "{}\n\n"
    "Entity Definition:\n"
    "1. corporation: Indicates named entities associated with corporations, companies or businesses.\n"
    "2. creative-work: Represents named entities related to creative works, such as books, movies, or songs.\n"
    "3. group: Indicates named entities associated with groups or organizations.\n"
    "4. location: Name of any geographic location, like cities, countries, continents, districts etc.\n"
    "5. person: Short name or full name of a person from any geographic regions.\n"
    "6. product: Denotes named entities associated with products or items.\n"
    "\n"
    "Output Format:\n"
    "'corporation': [list of entities present], 'creative-work': [list of entities present], 'group': [list of entities present],'location': [list of entities present],'person': [list of entities present],'product': [list of entities present]\n"
    "\n"
    "Examples:\n"
    "{}\n"
    "Now, generate Named Entity Recognition (NER) labels for the text related to {} domain for the following text:\n"
    "{}\n"
    "Output: "
)

import pinecone

pinecone.init(
	api_key='0a7ff5f2-254f-4aff-b399-0615282eaf8c',
	environment='us-central1-gcp'
)

index = pinecone.Index('wnut-ner-embed')

import random

"""
For our baseline, we plan to use randomly selected training sentences as examples
"""
def generate_examples_knn_task_mod(input_sentence, train_data, k=5, mode="label"):#, search_type="random"):
  embed = get_embedding(model, tokenizer, input_sentence)
  query = index.query(vector=embed, top_k=k)
  matches = query['matches']
  ids = [int(matches[i]['id'][5:]) for i in range(len(matches))]

  examples_str = ""

  for id in ids:
    examples_str += "Input: "
    examples_str += train_data[id]["sentence"]
    examples_str += "\n"
    examples_str += "Output: "
    examples_str += str(train_data[id]["entities"])
    examples_str += "\n\n"

  return examples_str
  #pass
#print(generate_examples_knn_task_mod(train_data["sentences"][0]["sentence"], train_data["sentences"], 5))

tags = test_data["tags"]
input_data = test_data["sentences"]
example_data = train_data["sentences"]
prompt = None
asnwer = None
with_domain_wnut = {"predictions":[], "labels":[]}
for i in range(len(input_data)):
  print(i)
  input = input_data[i]["sentence"]
  output = input_data[i]["entities"]
  examples = generate_examples_knn_task_mod(input,example_data, k=5)
  domain=get_domain(input)
  prompt = Prompt_domain.format(tags, examples, domain, input)
  answer = output
  generated_summary=generate_ner(prompt)
  print("answer:",answer)
  print("generated_summary",generated_summary)
  with_domain_wnut["predictions"].append(generated_summary)
  with_domain_wnut["labels"].append(answer)

with_domain_task_modification_wnut_result = json.dumps(with_domain_wnut, indent=2)
with open("with_domain_task_modification_wnut_result.json", 'w') as json_file:
    json_file.write(with_domain_task_modification_wnut_result)
