# -*- coding: utf-8 -*-
"""Baseline+KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RKcqKwjbocOKEpRBWv3e7MH00Diy9Umr

## Part 0: Setup
"""

!pip install openai

import openai
from openai import OpenAI

client = OpenAI(api_key = "")#, timeout=20.0)

import json

train_data = None

with open('conll_train.json', 'r') as infile:
  train_data = json.load(infile)

print(train_data["sentences"][0])

test_data = None

with open('conll_test.json', 'r') as infile:
  test_data = json.load(infile)

print(len(test_data["sentences"]))

SYSTEM_PROMPT = "You are a smart and intelligent Named Entity Recognition (NER) system."

USER_PROMPT_1 = "Are you clear about your role?"

ASSISTANT_PROMPT_1 = "Sure, I'm ready to help you with your NER task. Please provide me with the necessary information to get started."

template = (
    "Please generate Named Entity Recognition (NER) labels based on entity type for the last sentence, given the tags and examples below:\n\n"
    "Tags:\n"
    "{}\n\n"
    "Entity Definition:\n"
    "1. PER: Short name or full name of a person from any geographic regions.\n"
    "2. ORG: Name of the companies like Google, Samsung, Apple etc.\n"
    "3. LOC: Name of any geographic location, like cities, countries, continents, districts etc.\n"
    "4. MISC: Entities that don't fit into the above categories.\n"
    "\n"
    "Examples:\n"
    "{}"
    "Now, generate Named Entity Recognition (NER) labels for the following text:\n"
    "Input: {}\n"
    "Output: "
)

"""## Part 1: Baseline Results"""

import random

"""
For our baseline, we plan to use randomly selected training sentences as examples
"""
def generate_examples_random(train_data, k=5):#, search_type="random"):
  examples = random.choices(train_data, k=k)
  examples_str = ""

  for example in examples:
    examples_str += example["sentence"]
    examples_str += "\n"
    examples_str += example["ner_sentence"]
    examples_str += "\n\n"

  return examples_str

print(generate_examples_random(train_data["sentences"], 5))

from tqdm import tqdm

tags = test_data["tags"]
input_data = test_data["sentences"]

example_data = train_data["sentences"]

prompt = None
answer = None

baseline_test = {"predictions":[], "labels":[]}

for i in tqdm(range(len(input_data))):
  input = input_data[i]["sentence"]
  output = input_data[i]["ner_sentence"]

  examples = generate_examples_random(example_data, k=5)

  prompt = template.format(tags, examples, input)
  answer = output

  try:
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        temperature=0.5,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": USER_PROMPT_1},
            {"role": "assistant", "content": ASSISTANT_PROMPT_1},
            {"role": "user", "content": prompt},
        ]
    )

    generated_summary = vars(vars(vars(response)['choices'][0])['message'])['content'].strip()

    baseline_test["predictions"].append(generated_summary)
    baseline_test["labels"].append(answer)

  except:
    baseline_test["predictions"].append(None)
    baseline_test["labels"].append(answer)

print(len(baseline_test["predictions"]))

with open("baseline_conll_test.json", "w") as outfile:
  json.dump(baseline_test, outfile)

print(prompt)
print(answer)

predict = generated_summary.split(" ")
target = answer.split(" ")

print(len(predict))
print(len(target))

"""## Part 2: KNN Few Shot"""

!pip install pinecone-client transformers openai datasets

from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch

tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER")
model = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER")

def get_embedding(model, tokenizer, sentence):
    #sentence_embeddings = []
    model.eval()
    with torch.no_grad():
      inputs = tokenizer(sentence, return_tensors="pt")
      outputs = model(**inputs, output_hidden_states=True)
      token_vecs = outputs.hidden_states[-2][0]
      sentence_embedding = torch.mean(token_vecs, dim=0).tolist()
      #sentence_embeddings.append(sentence_embedding.tolist())

    return sentence_embedding

embed = get_embedding(model, tokenizer, "test")

import pinecone

pinecone.init(
    api_key="f8c61eed-169b-45a6-b16f-d9a3fb0daae8",
    environment="gcp-starter"
)

index = pinecone.Index('bert-ner-embed')

output = index.query(
  vector=embed,
  top_k=5
)

print(output['matches'][0]['id'][6:])
print(len("conll_"))

import random

"""
For our baseline, we plan to use randomly selected training sentences as examples
"""
def generate_examples_knn(input_sentence, train_data, k=5, mode="label"):#, search_type="random"):
  embed = get_embedding(model, tokenizer, input_sentence)
  query = index.query(vector=embed, top_k=k)
  matches = query['matches']
  ids = [int(matches[i]['id'][6:]) for i in range(len(matches))]

  examples_str = ""

  for id in ids:
    examples_str += "Input: "
    examples_str += train_data[id]["sentence"]
    examples_str += "\n"
    examples_str += "Output: "
    examples_str += train_data[id]["ner_sentence"]
    examples_str += "\n\n"

  return examples_str

  """
    examples_str = ""

    for example in examples:
      examples_str += example["sentence"]
      examples_str += "\n"
      examples_str += example["ner_sentence"]
      examples_str += "\n\n"

    return examples_str
  """
  #pass
print(generate_examples_knn("test", train_data["sentences"], 5))

from tqdm import tqdm

tags = test_data["tags"]
input_data = test_data["sentences"]

example_data = train_data["sentences"]

prompt = None
answer = None

knn_test = {"predictions":[], "labels":[]}

for i in tqdm(range(len(knn_test["predictions"]), len(input_data))):
  input = input_data[i]["sentence"]
  output = input_data[i]["ner_sentence"]

  examples = generate_examples_knn(input, example_data, k=5)

  prompt = template.format(tags, examples, input)
  answer = output

  try:
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        temperature=0.5,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": USER_PROMPT_1},
            {"role": "assistant", "content": ASSISTANT_PROMPT_1},
            {"role": "user", "content": prompt},
        ]
    )

    generated_summary = vars(vars(vars(response)['choices'][0])['message'])['content'].strip()

    knn_test["predictions"].append(generated_summary)
    knn_test["labels"].append(answer)

  except:
    pass

print(len(knn_test["predictions"]))

with open("knn_conll_test.json", "w") as outfile:
  json.dump(knn_test, outfile)

import random

"""
For our baseline, we plan to use randomly selected training sentences as examples
"""
def generate_examples_knn_task_mod(input_sentence, train_data, k=5, mode="label", dataset="wnut"):#, search_type="random"):
  embed = get_embedding(model, tokenizer, input_sentence)
  query = index.query(vector=embed, top_k=k)
  matches = query['matches']
  ids = [int(matches[i]['id'][len(dataset) + 1:]) for i in range(len(matches))]

  examples_str = ""

  for id in ids:
    examples_str += "Input: "
    examples_str += train_data[id]["sentence"]
    examples_str += "\n"
    examples_str += "Output: "
    examples_str += str(train_data[id]["entities"])
    examples_str += "\n\n"

  return examples_str

  """
    examples_str = ""

    for example in examples:
      examples_str += example["sentence"]
      examples_str += "\n"
      examples_str += example["ner_sentence"]
      examples_str += "\n\n"

    return examples_str
  """
  #pass
print(generate_examples_knn_task_mod(train_data["sentences"][0]["sentence"], train_data["sentences"], 5))
