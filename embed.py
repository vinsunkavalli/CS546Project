# -*- coding: utf-8 -*-
"""Copy of BERT_embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VO_aTvOKVMKtFGJi-noj62W4EQFHZQSA
"""

!pip install pinecone-client transformers openai datasets

from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch

tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER")
model = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER")

def get_embedding(model, tokenizer, sentences):
    sentence_embeddings = []
    model.eval()
    with torch.no_grad():
        for sentence in sentences:
            inputs = tokenizer(sentence, return_tensors="pt")
            outputs = model(**inputs, output_hidden_states=True)
            token_vecs = outputs.hidden_states[-2][0]
            sentence_embedding = torch.mean(token_vecs, dim=0)
            sentence_embeddings.append(sentence_embedding.tolist())
            # print(type(sentence_embeddings[0]))

    return sentence_embeddings

"""# Test 2: Pinecone Test"""

import pinecone

pinecone.init(
	api_key='',
	environment=''
)

index = pinecone.Index('wnut-ner-embed')

from datasets import load_dataset

wnut = load_dataset('wnut_17')
train = wnut["train"]

from tqdm import tqdm

batch_size = 128
sentences = []
for i in tqdm(range(len(train))):
  sentences.append(" ".join(train[i]["tokens"]))

  if i % batch_size == batch_size - 1:
    embeddings = get_embedding(model, tokenizer, sentences)
    id = ["wnut_" + str(j) for j in range(i - batch_size + 1, i + 1)]
    meta = [{'text':sentences[j]} for j in range(0, batch_size)]
    index_res = zip(id, embeddings, meta)
    index.upsert(list(index_res))

    sentences = []

